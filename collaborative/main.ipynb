{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea57ae6",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "Various global variables, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "DATASET_MAX_SIZE = 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1748c56",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This work is using the Million Song Dataset, specifically the one with user listenings count.\n",
    "It's available at http://millionsongdataset.com/tasteprofile/.\n",
    "> Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, & Paul Lamere (2011). The Million Song Dataset. In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011).\n",
    "\n",
    "It consists of a huge list (48m+ entries) of triplets `(user id, song id, listnings count)`.\n",
    "\n",
    "## Dataset parsing\n",
    "\n",
    "Read from the text file, simple parse and convert to numpy data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import numpy as np\n",
    "\n",
    "USER_MAPPING: dict[str, int] = {}\n",
    "SONG_MAPPING: dict[str, int] = {}\n",
    "\n",
    "# It's a list of tuples (user, song, listenings)\n",
    "dataset_raw: list[np.ndarray[tuple[Literal[3]], np.dtype[np.int32]]] = []\n",
    "with open(\"../../train_triplets.txt\", \"r\") as dataset_file:\n",
    "    for line in dataset_file:\n",
    "        user_id, song_id, listenings = line.split(\"\\t\")\n",
    "\n",
    "        line_vec = np.array(\n",
    "            [\n",
    "                USER_MAPPING.setdefault(user_id, len(USER_MAPPING)),\n",
    "                SONG_MAPPING.setdefault(song_id, len(SONG_MAPPING)),\n",
    "                int(listenings),\n",
    "            ]\n",
    "        )\n",
    "        dataset_raw.append(line_vec)\n",
    "\n",
    "        if len(dataset_raw) >= DATASET_MAX_SIZE:\n",
    "            break\n",
    "\n",
    "dataset = np.array(dataset_raw, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9c8b6",
   "metadata": {},
   "source": [
    "## Dataset training/validation preparation\n",
    "\n",
    "Shuffle & split into subsets.\n",
    "\n",
    "We take 2/3 for the training, and 1/3 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5624ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_means = np.mean(dataset[:, 2], axis=0)\n",
    "print(col_means)\n",
    "print(dataset[:, 2])\n",
    "print(col_means * np.ones(len(dataset[:, 2])))\n",
    "dataset[:, 2] = (dataset[:, 2] - col_means * np.ones(len(dataset[:, 2]))) / np.std(\n",
    "    dataset[:, 2], axis=0\n",
    ")\n",
    "print(dataset.dtype)\n",
    "print(dataset[:, 2])\n",
    "\n",
    "\n",
    "dataset_perm = np.random.permutation(len(dataset))\n",
    "dataset_shuffled = dataset[dataset_perm]\n",
    "training_set_size = int(len(dataset_shuffled) * 0.66)\n",
    "training_set = dataset_shuffled[:training_set_size]\n",
    "validation_set = dataset_shuffled[training_set_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfdcdf",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "The method used is a Stochastic Gradient Descent (SGD), using Regularized Mean Squared Error (RMSE) as the loss function.\n",
    "It corresponds to\n",
    "$$\n",
    "\\min_{q^*,p^*} \\sum_{(u,i) \\in \\mathcal{K}} \\left(r_{ui} - q_i^Tp_u\\right)^2 + \\lambda\\left(||q_i||^2 + ||p_u||^2\\right)\n",
    "$$\n",
    "\n",
    "The overall method is taken from [Matrix Factorization Techniques for Recommender Systems\n",
    "](https://ieeexplore.ieee.org/document/5197422).\n",
    "> Y. Koren, R. Bell and C. Volinsky, \"Matrix Factorization Techniques for Recommender Systems,\" in Computer, vol. 42, no. 8, pp. 30-37, Aug. 2009, doi: 10.1109/MC.2009.263. keywords: {Recommender systems;Motion pictures;Filtering;Collaboration;Sea measurements;Predictive models;Genomics;Bioinformatics;Nearest neighbor searches;Computational intelligence;Netflix Prize;Matrix factorization},\n",
    "\n",
    "\n",
    "\n",
    "## Prepare learning\n",
    "\n",
    "Prepare learning sets $q$ and $p$, which are random matrices of shapes $(|\\mathrm{songs}|, l)$ and $(|\\mathrm{users}|, l)$.\n",
    "\n",
    "Set parameters:\n",
    "- Size `l` of latent space to embed users & films\n",
    "- Learning rate $\\gamma$\n",
    "- Regularization $\\lambda$\n",
    "- Number of epochs (rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77343f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Hyperparameter) Size of latent space to make the embeddings\n",
    "l = 1000\n",
    "# Initial (random) values\n",
    "# Shape: (#SONGS, l)\n",
    "q = np.random.random_sample((len(SONG_MAPPING), l))\n",
    "if DEBUG:\n",
    "    print(q.shape, q.dtype, q)\n",
    "# Shape: (#USERS, l)\n",
    "p = np.random.random_sample((len(USER_MAPPING), l))\n",
    "if DEBUG:\n",
    "    print(p.shape, p.dtype, p)\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "lbd = 0.01\n",
    "gamma = 0.01\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29bdfe",
   "metadata": {},
   "source": [
    "## Actual learning\n",
    "\n",
    "Process the SGD, accumulating loss so it can be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [np.nan] * n_epochs\n",
    "losses_validation = [np.nan] * n_epochs\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    loss_sum: float = 0\n",
    "\n",
    "    np.random.shuffle(training_set)  # Reorder each epoch\n",
    "    # user \\in [0, #USERS - 1]\n",
    "    # song \\in [0, #SONGS - 1]\n",
    "    # listenings \\in N (r_ui, \"true\" value)\n",
    "    for i, (user, song, listenings) in enumerate(training_set):\n",
    "        if DEBUG:\n",
    "            print(\n",
    "                f\"Training value {i}/{len(training_set)}: ({user},{song},{listenings})\"\n",
    "            )\n",
    "\n",
    "        # Predicted value\n",
    "        p_u = p[user].copy()\n",
    "        q_i = q[song].copy()\n",
    "        if DEBUG:\n",
    "            print(p_u)\n",
    "            print(q_i)\n",
    "\n",
    "        listenings_hat = p_u.T @ q_i\n",
    "        if DEBUG:\n",
    "            print(f\"Prediction: {listenings_hat}\")\n",
    "\n",
    "        # Prediction error\n",
    "        e_ui = listenings - listenings_hat\n",
    "\n",
    "        # This is the learning part\n",
    "        q[song] += gamma * (e_ui * p_u - lbd * q_i)\n",
    "        p[user] += gamma * (e_ui * q_i - lbd * p_u)\n",
    "\n",
    "        # Loss\n",
    "        loss = e_ui**2 + lbd * (np.linalg.norm(q_i) ** 2 + np.linalg.norm(p_u) ** 2)\n",
    "        if DEBUG:\n",
    "            print(f\"Loss: {loss}\")\n",
    "        loss_sum += loss\n",
    "\n",
    "    losses[epoch] = loss_sum / len(training_set)\n",
    "\n",
    "    # Now evaluating on validation data\n",
    "    loss_validation_sum = 0\n",
    "    for user, song, listenings in validation_set:\n",
    "        listenings_hat = p[user].T @ q[song]\n",
    "\n",
    "        e_ui = listenings - listenings_hat\n",
    "\n",
    "        # Loss\n",
    "        loss = e_ui**2 + lbd * (\n",
    "            np.linalg.norm(q[song]) ** 2 + np.linalg.norm(p[user]) ** 2\n",
    "        )\n",
    "        loss_validation_sum += loss\n",
    "\n",
    "    losses_validation[epoch] = loss_validation_sum / len(training_set)\n",
    "\n",
    "    print(f\"Loss: {losses[epoch]}, validation loss: {losses_validation[epoch]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50658c26",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Learning results\n",
    "\n",
    "We first analyze the learning raw results: training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b43224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "_ = ax.plot(losses, label=\"Train loss\")\n",
    "_ = ax.plot(losses_validation, label=\"Validation loss\")\n",
    "_ = ax.set_yscale(\"log\")\n",
    "_ = ax.set_xlabel(\"epoch\")\n",
    "_ = ax.set_title(\"Losses during learning\")\n",
    "_ = fig.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a4488",
   "metadata": {},
   "source": [
    "## ... more analysis\n",
    "\n",
    "Evaluation of the model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
