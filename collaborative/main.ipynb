{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea57ae6",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "Various global variables, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "DATASET_MAX_SIZE = 1_000_000\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1748c56",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This work is using the Million Song Dataset, specifically the one with user listenings count.\n",
    "It's available at http://millionsongdataset.com/tasteprofile/.\n",
    "> Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, & Paul Lamere (2011). The Million Song Dataset. In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011).\n",
    "\n",
    "It consists of a huge list (48m+ entries) of triplets `(user id, song id, listnings count)`.\n",
    "\n",
    "## Dataset parsing\n",
    "\n",
    "Read from the text file, simple parse and convert to numpy data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps users and songs to their unique index for further referencing as matrix index\n",
    "USER_MAPPING: dict[str, int] = {}\n",
    "SONG_MAPPING: dict[str, int] = {}\n",
    "\n",
    "# It's a list of tuples (user, song, listening count)\n",
    "dataset_triplet_dtype = np.dtype(\n",
    "    [\n",
    "        (\"Song index\", np.uint32),\n",
    "        (\"User index\", np.uint32),\n",
    "        (\"Listening count\", np.float64),\n",
    "    ]\n",
    ")\n",
    "dataset_raw: list[tuple[int, int, int]] = []\n",
    "with open(\"../train_triplets.txt\", \"r\") as dataset_file:\n",
    "    for line in dataset_file:\n",
    "        user_id, song_id, listening_count = line.split(\"\\t\")\n",
    "\n",
    "        dataset_raw.append(\n",
    "            (\n",
    "                USER_MAPPING.setdefault(user_id, len(USER_MAPPING)),\n",
    "                SONG_MAPPING.setdefault(song_id, len(SONG_MAPPING)),\n",
    "                int(listening_count),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if len(dataset_raw) >= DATASET_MAX_SIZE:\n",
    "            break\n",
    "\n",
    "dataset = np.array(dataset_raw, dtype=dataset_triplet_dtype)\n",
    "\n",
    "print(dataset.dtype)\n",
    "print(dataset)\n",
    "print(\n",
    "    f\"Parsed {len(SONG_MAPPING)} and {len(USER_MAPPING)} users, for a total of {len(dataset)} triplets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311f8de",
   "metadata": {},
   "source": [
    "## Dataset normalization\n",
    "\n",
    "Standardize: remove mean and divide by standard deviation.\n",
    "Hence, values are unitless, centered around 0, and spans in $[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Listening count\"] = (\n",
    "    dataset[\"Listening count\"] - dataset[\"Listening count\"].mean()\n",
    ") / dataset[\"Listening count\"].std()\n",
    "\n",
    "print(dataset.dtype, dataset.shape)\n",
    "print(dataset[\"Listening count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9c8b6",
   "metadata": {},
   "source": [
    "## Dataset training/validation preparation\n",
    "\n",
    "Shuffle & split into subsets.\n",
    "\n",
    "We take 2/3 for the training, and 1/3 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5624ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_size = int(len(dataset) * 0.66)\n",
    "\n",
    "dataset_perm = np.random.permutation(len(dataset))\n",
    "dataset_shuffled = dataset[dataset_perm]\n",
    "\n",
    "train_set = dataset_shuffled[:training_set_size]\n",
    "validation_set = dataset_shuffled[training_set_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfdcdf",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "The method used is a Stochastic Gradient Descent (SGD), using Regularized Mean Squared Error (RMSE) as the loss function.\n",
    "It corresponds to\n",
    "$$\n",
    "\\min_{q^*,p^*} \\sum_{(u,i) \\in \\mathcal{K}} \\left(r_{ui} - q_i^Tp_u\\right)^2 + \\lambda\\left(||q_i||^2 + ||p_u||^2\\right)\n",
    "$$\n",
    "\n",
    "The overall method is taken from [Matrix Factorization Techniques for Recommender Systems\n",
    "](https://ieeexplore.ieee.org/document/5197422).\n",
    "> Y. Koren, R. Bell and C. Volinsky, \"Matrix Factorization Techniques for Recommender Systems,\" in Computer, vol. 42, no. 8, pp. 30-37, Aug. 2009, doi: 10.1109/MC.2009.263. keywords: {Recommender systems;Motion pictures;Filtering;Collaboration;Sea measurements;Predictive models;Genomics;Bioinformatics;Nearest neighbor searches;Computational intelligence;Netflix Prize;Matrix factorization},\n",
    "\n",
    "\n",
    "\n",
    "## Prepare learning\n",
    "\n",
    "Prepare learning sets $q$ and $p$, which are random matrices of shapes $(|\\mathrm{songs}|, l)$ and $(|\\mathrm{users}|, l)$.\n",
    "\n",
    "Set parameters:\n",
    "- Size `l` of latent space to embed users & films\n",
    "- Learning rate $\\gamma$\n",
    "- Regularization $\\lambda$\n",
    "- Number of epochs (rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77343f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Hyperparameter) Size of latent space to make the embeddings\n",
    "l = 100\n",
    "# Initial (random) values\n",
    "# Shape: (#SONGS, l)\n",
    "q = np.random.random_sample((len(SONG_MAPPING), l))\n",
    "if DEBUG:\n",
    "    print(q.shape, q.dtype, q)\n",
    "# Shape: (#USERS, l)\n",
    "p = np.random.random_sample((len(USER_MAPPING), l))\n",
    "if DEBUG:\n",
    "    print(p.shape, p.dtype, p)\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "lbd = 0.001\n",
    "gamma = 0.0005\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29bdfe",
   "metadata": {},
   "source": [
    "## Actual learning\n",
    "\n",
    "Process the SGD, accumulating loss so it can be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from typing import TypedDict, cast\n",
    "\n",
    "\n",
    "class LearningStats(TypedDict):\n",
    "    \"\"\"\n",
    "    Learning stats (losses: train & validation) for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    losses_train: list[np.float64 | float]\n",
    "    losses_validation: list[np.float64 | float]\n",
    "\n",
    "\n",
    "def train(l: int, lbd: float, gamma: float, n_epochs: int) -> LearningStats:\n",
    "    learning_stats: LearningStats = {\n",
    "        \"losses_train\": [np.nan] * n_epochs,\n",
    "        \"losses_validation\": [np.nan] * n_epochs,\n",
    "    }\n",
    "\n",
    "    print(f\"Training with l={l}, lambda={lbd}, gamma={gamma} for {n_epochs} epochs.\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        loss_sum: float = 0\n",
    "\n",
    "        np.random.shuffle(train_set)  # Reorder each epoch\n",
    "        # user \\in [0, #USERS - 1]\n",
    "        # song \\in [0, #SONGS - 1]\n",
    "        # listenings \\in N (r_ui, \"true\" value)\n",
    "        for i, (user, song, listening_count) in enumerate(\n",
    "            cast(Iterable[tuple[np.uint32, np.uint32, np.float64]], train_set)\n",
    "        ):\n",
    "            if DEBUG:\n",
    "                print(\n",
    "                    f\"Training value {i}/{len(train_set)}: ({user},{song},{listening_count})\"\n",
    "                )\n",
    "\n",
    "            # Predicted value\n",
    "            p_u = p[user].copy()\n",
    "            q_i = q[song].copy()\n",
    "            if DEBUG:\n",
    "                print(p_u)\n",
    "                print(q_i)\n",
    "\n",
    "            listenings_hat = p_u.T @ q_i\n",
    "            if DEBUG:\n",
    "                print(f\"Prediction: {listenings_hat}\")\n",
    "\n",
    "            # Prediction error\n",
    "            e_ui = listening_count - listenings_hat\n",
    "\n",
    "            # This is the learning part\n",
    "            q[song] += gamma * (e_ui * p_u - lbd * q_i)\n",
    "            p[user] += gamma * (e_ui * q_i - lbd * p_u)\n",
    "\n",
    "            # Loss\n",
    "            loss = e_ui**2 + lbd * (np.linalg.norm(q_i) ** 2 + np.linalg.norm(p_u) ** 2)\n",
    "            if DEBUG:\n",
    "                print(f\"Loss: {loss}\")\n",
    "            loss_sum += loss\n",
    "\n",
    "        learning_stats[\"losses_train\"][epoch] = loss_sum / len(train_set)\n",
    "\n",
    "        # Now evaluating on validation data\n",
    "        loss_validation_sum = 0\n",
    "        for user, song, listening_count in validation_set:\n",
    "            listenings_hat = p[user].T @ q[song]\n",
    "\n",
    "            e_ui = listening_count - listenings_hat\n",
    "\n",
    "            # Loss\n",
    "            loss = e_ui**2 + lbd * (\n",
    "                np.linalg.norm(q[song]) ** 2 + np.linalg.norm(p[user]) ** 2\n",
    "            )\n",
    "            loss_validation_sum += loss\n",
    "\n",
    "        learning_stats[\"losses_validation\"][epoch] = loss_validation_sum / len(\n",
    "            train_set\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Loss (train): {learning_stats['losses_train'][epoch]}, loss (validation): {learning_stats['losses_validation'][epoch]}\"\n",
    "        )\n",
    "\n",
    "    return learning_stats\n",
    "\n",
    "training_stats = train(l, lbd, gamma, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50658c26",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Learning results\n",
    "\n",
    "We first analyze the learning raw results: training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b43224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "_ = ax.plot(training_stats[\"losses_train\"], label=\"Train loss\")\n",
    "_ = ax.plot(training_stats[\"losses_validation\"], label=\"Validation loss\")\n",
    "_ = ax.set_yscale(\"log\")\n",
    "_ = ax.set_xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "_ = ax.set_ylabel(\"Loss\")\n",
    "_ = ax.set_title(\"Losses during learning\")\n",
    "_ = fig.legend()\n",
    "plt.close(fig)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a4488",
   "metadata": {},
   "source": [
    "## ... more analysis\n",
    "\n",
    "Evaluation of the model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
