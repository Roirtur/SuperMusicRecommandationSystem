"""
Systeme de Recommandation Musical Content-Based
avec Evaluation Precision@k et Recall@k
Dataset: Million Song Subset
Methodes: Sentence Transformer + Clustering + Moyenne ponderee
"""

import os
import sys
import subprocess
import importlib

# ============================================================================
# INSTALLATION AUTOMATIQUE DES DEPENDANCES
# ============================================================================

def install_and_import_packages():
    """Verifie et installe les packages manquants automatiquement"""
    
    required_packages = [
        'pandas',
        'numpy', 
        'scikit-learn',
        'sentence-transformers',
        'matplotlib',
        'seaborn',
        'tqdm',
        'requests'
    ]
    
    print("Verification des packages requis...")
    print("=" * 60)
    
    for package in required_packages:
        try:
            # Essayer d'importer le package
            importlib.import_module(package.replace('-', '_'))
            print(f"OK : {package}")
        except ImportError:
            print(f"Manquant : {package}")
            print(f"Installation de {package}...")
            
            try:
                # Essayer d'installer avec pip
                subprocess.check_call([sys.executable, "-m", "pip", "install", package])
                print(f"  -> {package} installe avec succes")
            except subprocess.CalledProcessError:
                print(f"  -> Echec de l'installation de {package}")
                print(f"  -> Veuillez installer manuellement: pip install {package}")
                
                # Pour scikit-learn, essayer l'installation specifique
                if package == 'scikit-learn':
                    print("  -> Tentative alternative pour scikit-learn...")
                    try:
                        subprocess.check_call([sys.executable, "-m", "pip", "install", "scikit-learn==1.3.0"])
                        print("  -> scikit-learn installe avec succes")
                    except:
                        print("  -> Echec. Essayez: pip install scikit-learn")
                        sys.exit(1)
    
    print("\n" + "=" * 60)
    print("Tous les packages sont prets!")
    print("=" * 60 + "\n")

# Executer l'installation avant d'importer
install_and_import_packages()

# ============================================================================
# IMPORT DES PACKAGES (apres installation)
# ============================================================================

import numpy as np
import pandas as pd
import sqlite3
import requests
import tarfile
import json
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# 1. GESTION DU DATASET
# ============================================================================

class MSDDownloader:
    """Telecharge et prepare le dataset Million Song Subset"""
    
    SUBSET_URL = "http://static.echonest.com/millionsongsubset_full.tar.gz"
    METADATA_URL = "http://millionsongdataset.com/sites/default/files/additional_files/track_metadata.db"
    
    def __init__(self, data_dir="msd_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
    
    def download_file(self, url, filename):
        """Telecharge un fichier avec progression"""
        filepath = self.data_dir / filename
        
        if filepath.exists():
            print(f"Fichier {filename} deja present")
            return filepath
        
        try:
            print(f"Telechargement de {filename}...")
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            with open(filepath, 'wb') as f, tqdm(
                desc=filename,
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
            
            print(f"Fichier {filename} telecharge")
            return filepath
        except Exception as e:
            print(f"Erreur de telechargement: {e}")
            print("Creation de donnees de test a la place...")
            return None
    
    def extract_subset(self):
        """Extrait l'archive du dataset"""
        tar_path = self.data_dir / "millionsongsubset_full.tar.gz"
        
        if tar_path and tar_path.exists():
            print("Extraction de l'archive...")
            try:
                with tarfile.open(tar_path, 'r:gz') as tar:
                    tar.extractall(self.data_dir)
                print("Extraction terminee")
                return True
            except Exception as e:
                print(f"Erreur d'extraction: {e}")
                return False
        return False
    
    def setup(self):
        """Configure le dataset complet"""
        print("Configuration du Million Song Subset")
        print("=" * 70)
        
        self.download_file(self.SUBSET_URL, "millionsongsubset_full.tar.gz")
        self.extract_subset()
        
        self.download_file(self.METADATA_URL, "track_metadata.db")
        
        return self.data_dir

# ============================================================================
# 2. PREPARATION DES DONNEES
# ============================================================================

class DataPreparer:
    """Prepare les donnees pour le traitement"""
    
    def __init__(self, data_dir):
        self.data_dir = Path(data_dir)
    
    def load_metadata(self, limit=3000):
        """Charge les metadonnees depuis la base SQLite"""
        db_path = self.data_dir / "track_metadata.db"
        
        if not db_path.exists():
            print("Base de donnees non trouvee, creation de donnees de test...")
            return self._create_test_data(limit)
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            
            for table_name, _ in tables:
                try:
                    query = f"SELECT * FROM {table_name} LIMIT {limit}"
                    df = pd.read_sql_query(query, conn)
                    
                    if not df.empty and 'title' in df.columns and 'artist_name' in df.columns:
                        print(f"Donnees chargees depuis {table_name}: {len(df)} chansons")
                        
                        if 'track_id' not in df.columns and 'tid' in df.columns:
                            df = df.rename(columns={'tid': 'track_id'})
                        if 'song_id' not in df.columns and 'track_id' in df.columns:
                            df['song_id'] = df['track_id']
                        
                        conn.close()
                        return df
                except:
                    continue
            
            conn.close()
            return self._create_test_data(limit)
            
        except Exception as e:
            print(f"Erreur SQLite: {e}")
            return self._create_test_data(limit)
    
    def _create_test_data(self, n_songs):
        """Cree des donnees de test simulees"""
        print(f"Creation de {n_songs} chansons de test...")
        
        artists = ["The Beatles", "Queen", "Michael Jackson", "Madonna", "U2",
                  "Beyonce", "Coldplay", "Radiohead", "Taylor Swift", "Drake"]
        genres = ["Rock", "Pop", "Hip Hop", "R&B", "Jazz", "Electronic"]
        moods = ["energique", "melancolique", "joyeux", "romantique", "dansant", "calme"]
        
        data = {
            'track_id': [f"TR{i:06d}" for i in range(n_songs)],
            'song_id': [f"SO{i:06d}" for i in range(n_songs)],
            'title': [f"Chanson {i}" for i in range(n_songs)],
            'artist_name': np.random.choice(artists, n_songs),
            'year': np.random.randint(1960, 2023, n_songs),
            'duration': np.random.randint(120, 420, n_songs),
            'tempo': np.random.randint(70, 180, n_songs),
            'loudness': np.random.uniform(-20, 0, n_songs),
            'artist_hotttnesss': np.random.uniform(0.1, 1.0, n_songs),
            'song_hotttnesss': np.random.uniform(0.1, 1.0, n_songs),
            'genre': np.random.choice(genres, n_songs),
            'mood': np.random.choice(moods, n_songs)
        }
        
        return pd.DataFrame(data)
    
    def create_descriptions(self, df):
        """Cree des descriptions textuelles pour les chansons"""
        print("Creation des descriptions textuelles...")
        
        def build_description(row):
            parts = []
            
            parts.append(f"Titre: {row.get('title', 'Inconnu')}")
            parts.append(f"Artiste: {row.get('artist_name', 'Inconnu')}")
            
            if pd.notna(row.get('year')):
                parts.append(f"Annee: {int(row.get('year'))}")
            
            if pd.notna(row.get('genre')):
                parts.append(f"Genre: {row.get('genre')}")
            
            if pd.notna(row.get('mood')):
                parts.append(f"Humeur: {row.get('mood')}")
            
            if pd.notna(row.get('tempo')):
                tempo = int(row.get('tempo'))
                if tempo > 140:
                    parts.append("Tempo rapide")
                elif tempo < 100:
                    parts.append("Tempo lent")
                else:
                    parts.append("Tempo modere")
            
            if pd.notna(row.get('duration')):
                mins = int(row.get('duration')) // 60
                secs = int(row.get('duration')) % 60
                parts.append(f"Duree: {mins}min{secs}sec")
            
            if pd.notna(row.get('artist_hotttnesss')):
                hotness = row.get('artist_hotttnesss')
                if hotness > 0.8:
                    parts.append("Artiste tres populaire")
                elif hotness > 0.5:
                    parts.append("Artiste populaire")
            
            if pd.notna(row.get('song_hotttnesss')):
                if row.get('song_hotttnesss') > 0.8:
                    parts.append("Chanson tendance")
            
            return ". ".join(parts)
        
        df['description'] = df.apply(build_description, axis=1)
        
        print(f"Descriptions creees pour {len(df)} chansons")
        return df

# ============================================================================
# 3. GENERATION DES EMBEDDINGS
# ============================================================================

class EmbeddingGenerator:
    """Genere les embeddings avec Sentence Transformer"""
    
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        try:
            from sentence_transformers import SentenceTransformer
            print(f"Chargement du modele {model_name}...")
            self.model = SentenceTransformer(model_name)
            print("Modele charge avec succes")
        except ImportError:
            print("Erreur: sentence_transformers non installe")
            print("Installer avec: pip install sentence-transformers")
            sys.exit(1)
    
    def generate_embeddings(self, songs_df, batch_size=32):
        """Genere les embeddings pour toutes les chansons"""
        print(f"Generation des embeddings pour {len(songs_df)} chansons...")
        
        descriptions = songs_df['description'].tolist()
        
        embeddings = []
        for i in tqdm(range(0, len(descriptions), batch_size), 
                     desc="Embeddings", unit="batch"):
            batch = descriptions[i:i+batch_size]
            batch_embeddings = self.model.encode(
                batch, 
                convert_to_numpy=True,
                show_progress_bar=False
            )
            embeddings.append(batch_embeddings)
        
        song_embeddings = np.vstack(embeddings)
        
        song_id_to_embedding = {
            row['track_id']: song_embeddings[i] 
            for i, (_, row) in enumerate(songs_df.iterrows())
        }
        
        print(f"Embeddings generes: {song_embeddings.shape}")
        return song_embeddings, song_id_to_embedding

# ============================================================================
# 4. SYSTEME DE RECOMMANDATION
# ============================================================================

class MusicRecommender:
    """Systeme de recommandation musical hybride"""
    
    def __init__(self, songs_df, song_embeddings, song_id_to_embedding):
        self.songs_df = songs_df.copy()
        self.song_embeddings = song_embeddings
        self.song_id_to_embedding = song_id_to_embedding
        self.song_id_to_idx = {tid: i for i, tid in enumerate(songs_df['track_id'])}
        
        # Clustering des chansons
        self._perform_clustering()
    
    def _perform_clustering(self):
        """Effectue le clustering des chansons"""
        print("Clustering des chansons...")
        n_clusters = min(50, len(self.songs_df) // 10)
        if n_clusters < 5:
            n_clusters = 5
            
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(self.song_embeddings)
        self.songs_df['cluster'] = cluster_labels
        self.cluster_centers = kmeans.cluster_centers_
        
        # Index cluster -> chansons
        self.cluster_to_songs = {}
        for _, row in self.songs_df.iterrows():
            cluster = row['cluster']
            song_id = row['track_id']
            
            if cluster not in self.cluster_to_songs:
                self.cluster_to_songs[cluster] = []
            self.cluster_to_songs[cluster].append(song_id)
    
    def create_user_profile(self, user_history):
        """Cree le profil utilisateur avec moyenne ponderee"""
        valid_items = []
        total_views = 0
        
        for song_id, views in user_history:
            if song_id in self.song_id_to_embedding:
                try:
                    views_num = float(views)
                    if views_num > 0:
                        valid_items.append((song_id, views_num))
                        total_views += views_num
                except:
                    continue
        
        if not valid_items:
            return None
        
        # Moyenne ponderee
        weighted_sum = None
        for song_id, views in valid_items:
            embedding = self.song_id_to_embedding[song_id]
            if weighted_sum is None:
                weighted_sum = embedding * views
            else:
                weighted_sum += embedding * views
        
        user_embedding = weighted_sum / total_views
        
        return {
            'embedding': user_embedding,
            'history': valid_items,
            'total_views': total_views,
            'song_ids': [song_id for song_id, _ in valid_items]
        }
    
    def recommend(self, user_profile, n_recommend=10, method='hybrid', alpha=0.6):
        """Genere des recommandations"""
        if user_profile is None:
            return pd.DataFrame()
        
        if method == 'similarity':
            recommendations = self._recommend_by_similarity(user_profile, n_recommend)
        elif method == 'cluster':
            recommendations = self._recommend_by_cluster(user_profile, n_recommend)
        else:  # hybrid
            recommendations = self._hybrid_recommendation(user_profile, n_recommend, alpha)
        
        return recommendations
    
    def _recommend_by_similarity(self, user_profile, n_recommend):
        """Recommandation par similarite cosinus"""
        similarities = cosine_similarity(
            user_profile['embedding'].reshape(1, -1),
            self.song_embeddings
        )[0]
        
        results_df = pd.DataFrame({
            'track_id': self.songs_df['track_id'],
            'similarity': similarities
        })
        
        # Exclure les chansons deja ecoutees
        results_df = results_df[~results_df['track_id'].isin(user_profile['song_ids'])]
        
        # Meilleures recommandations
        recommendations = results_df.sort_values('similarity', ascending=False).head(n_recommend)
        
        # Ajouter les informations
        recommendations = recommendations.merge(
            self.songs_df[['track_id', 'title', 'artist_name', 'genre', 'cluster']],
            on='track_id',
            how='left'
        )
        
        return recommendations
    
    def _recommend_by_cluster(self, user_profile, n_recommend):
        """Recommandation par clustering"""
        # Clusters des chansons aimees
        liked_clusters = set()
        for song_id in user_profile['song_ids']:
            if song_id in self.song_id_to_idx:
                idx = self.song_id_to_idx[song_id]
                liked_clusters.add(self.songs_df.iloc[idx]['cluster'])
        
        # Chansons de ces clusters
        cluster_songs = []
        for cluster in liked_clusters:
            if cluster in self.cluster_to_songs:
                for song_id in self.cluster_to_songs[cluster]:
                    if song_id not in user_profile['song_ids']:
                        cluster_songs.append(song_id)
        
        # Prendre n_recommend chansons uniques
        unique_songs = []
        for song_id in cluster_songs:
            if song_id not in unique_songs:
                unique_songs.append(song_id)
            if len(unique_songs) >= n_recommend:
                break
        
        # DataFrame de resultats
        recommendations = pd.DataFrame({'track_id': unique_songs})
        recommendations = recommendations.merge(
            self.songs_df[['track_id', 'title', 'artist_name', 'genre', 'cluster']],
            on='track_id',
            how='left'
        )
        
        return recommendations
    
    def _hybrid_recommendation(self, user_profile, n_recommend, alpha=0.6):
        """Recommandation hybride"""
        n_similarity = int(n_recommend * alpha)
        n_cluster = n_recommend - n_similarity
        
        # Recommandations par similarite
        similarity_recs = self._recommend_by_similarity(user_profile, n_similarity * 2)
        
        # Recommandations par cluster
        cluster_recs = self._recommend_by_cluster(user_profile, n_cluster * 2)
        
        # Combiner
        combined_ids = []
        
        # Ajouter d'abord par similarite
        for song_id in similarity_recs['track_id'].tolist():
            if song_id not in combined_ids:
                combined_ids.append(song_id)
            if len(combined_ids) >= n_recommend:
                break
        
        # Completer avec les clusters
        for song_id in cluster_recs['track_id'].tolist():
            if song_id not in combined_ids:
                combined_ids.append(song_id)
            if len(combined_ids) >= n_recommend:
                break
        
        # DataFrame final
        recommendations = pd.DataFrame({'track_id': combined_ids[:n_recommend]})
        recommendations = recommendations.merge(
            self.songs_df[['track_id', 'title', 'artist_name', 'genre', 'cluster']],
            on='track_id',
            how='left'
        )
        
        # Ajouter les scores de similarite
        if user_profile is not None:
            similarities = []
            for song_id in recommendations['track_id']:
                if song_id in self.song_id_to_embedding:
                    sim = cosine_similarity(
                        user_profile['embedding'].reshape(1, -1),
                        self.song_id_to_embedding[song_id].reshape(1, -1)
                    )[0][0]
                    similarities.append(sim)
                else:
                    similarities.append(0)
            
            recommendations['similarity_score'] = similarities
        
        return recommendations

# ============================================================================
# 5. EVALUATION AVEC PRECISION@k ET RECALL@k
# ============================================================================

class SystemEvaluator:
    """
    Evaluation du systeme avec Precision@k et Recall@k
    
    Definitions:
    Precision@k = (chansons pertinentes dans top-k) / k
    Recall@k = (chansons pertinentes dans top-k) / (total chansons pertinentes)
    
    Pourquoi evaluer avec k?
    1. Realisme: les utilisateurs ne regardent que les premieres recommandations
    2. Impact pratique: seules les premieres positions ont de la valeur
    3. Reduit le biais: sans limite, les systemes pourraient recommander
       beaucoup d'items non pertinents pour ameliorer artificiellement les scores
    """
    
    def __init__(self, recommender, songs_df):
        self.recommender = recommender
        self.songs_df = songs_df
    
    def generate_test_users(self, n_users=50, min_tracks=5, max_tracks=15, test_ratio=0.3):
        """
        Genere des utilisateurs de test avec split train/test
        """
        print(f"Generation de {n_users} utilisateurs de test...")
        
        test_users = []
        all_song_ids = self.songs_df['track_id'].tolist()
        
        for i in range(n_users):
            n_tracks = np.random.randint(min_tracks, max_tracks + 1)
            selected_ids = np.random.choice(all_song_ids, n_tracks, replace=False)
            
            views = np.random.randint(1, 101, n_tracks)
            
            # Split 70% train, 30% test
            split_idx = int(n_tracks * (1 - test_ratio))
            
            indices = np.arange(n_tracks)
            np.random.shuffle(indices)
            
            train_indices = indices[:split_idx]
            test_indices = indices[split_idx:]
            
            train_history = [(selected_ids[idx], views[idx]) for idx in train_indices]
            test_songs = [selected_ids[idx] for idx in test_indices]
            
            test_users.append({
                'user_id': f"user_{i:03d}",
                'train_history': train_history,
                'test_songs': test_songs,
                'all_songs': selected_ids.tolist()
            })
        
        print(f"Utilisateurs generes: {len(test_users)}")
        print(f"Chansons par utilisateur: {split_idx} train, {n_tracks - split_idx} test")
        
        return test_users
    
    def evaluate_precision_recall_at_k(self, test_users, k_values=[5, 10, 15, 20], alpha=0.6):
        """
        Evalue Precision@k et Recall@k pour differentes valeurs de k
        """
        print("\n" + "="*80)
        print("EVALUATION PRECISION@k ET RECALL@k")
        print("="*80)
        
        results = {}
        
        for k in k_values:
            print(f"\nEvaluation pour k = {k}")
            print("-"*50)
            
            precisions = []
            recalls = []
            f1_scores = []
            
            for user_data in tqdm(test_users, desc=f"Utilisateurs (k={k})"):
                try:
                    # Profil utilisateur avec donnees d'entrainement
                    user_profile = self.recommender.create_user_profile(user_data['train_history'])
                    
                    if user_profile is None:
                        continue
                    
                    # Generation des recommandations
                    recommendations = self.recommender.recommend(
                        user_profile, 
                        n_recommend=k,
                        method='hybrid',
                        alpha=alpha
                    )
                    
                    if recommendations.empty:
                        continue
                    
                    # Top-k recommandations
                    top_k_recommendations = recommendations['track_id'].head(k).tolist()
                    
                    # Chansons de test (pertinentes)
                    test_songs = user_data['test_songs']
                    
                    # Calcul Precision@k et Recall@k
                    relevant_recommended = len(set(top_k_recommendations) & set(test_songs))
                    
                    precision_at_k = relevant_recommended / k if k > 0 else 0
                    recall_at_k = relevant_recommended / len(test_songs) if test_songs else 0
                    
                    # Calcul F1-score@k
                    if precision_at_k + recall_at_k > 0:
                        f1_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)
                    else:
                        f1_at_k = 0
                    
                    precisions.append(precision_at_k)
                    recalls.append(recall_at_k)
                    f1_scores.append(f1_at_k)
                    
                except Exception:
                    continue
            
            # Calcul des moyennes
            if precisions and recalls:
                avg_precision = np.mean(precisions)
                avg_recall = np.mean(recalls)
                avg_f1 = np.mean(f1_scores)
                
                std_precision = np.std(precisions)
                std_recall = np.std(recalls)
                std_f1 = np.std(f1_scores)
                
                results[k] = {
                    'precision': avg_precision,
                    'recall': avg_recall,
                    'f1': avg_f1,
                    'std_precision': std_precision,
                    'std_recall': std_recall,
                    'std_f1': std_f1,
                    'n_users': len(precisions)
                }
                
                print(f"Resultats pour k={k}:")
                print(f"  Precision@{k}: {avg_precision:.3f} (±{std_precision:.3f})")
                print(f"  Recall@{k}:    {avg_recall:.3f} (±{std_recall:.3f})")
                print(f"  F1-score@{k}:  {avg_f1:.3f} (±{std_f1:.3f})")
                print(f"  Utilisateurs evalues: {len(precisions)}")
            else:
                print(f"Aucune donnee pour k={k}")
        
        return results
    
    def plot_results(self, results, title="Evaluation du Systeme"):
        """Visualise les resultats d'evaluation"""
        if not results:
            print("Aucun resultat a visualiser")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(title, fontsize=16)
        
        # Extraire les donnees
        k_values = sorted(results.keys())
        precisions = [results[k]['precision'] for k in k_values]
        recalls = [results[k]['recall'] for k in k_values]
        f1_scores = [results[k]['f1'] for k in k_values]
        n_users = [results[k]['n_users'] for k in k_values]
        
        # Graphique 1: Precision@k
        axes[0, 0].plot(k_values, precisions, 'o-', linewidth=2, markersize=8, color='blue')
        axes[0, 0].fill_between(k_values, 
                               [p - results[k]['std_precision'] for k, p in zip(k_values, precisions)],
                               [p + results[k]['std_precision'] for k, p in zip(k_values, precisions)],
                               alpha=0.2, color='blue')
        axes[0, 0].set_xlabel('k (nombre de recommandations)')
        axes[0, 0].set_ylabel('Precision@k')
        axes[0, 0].set_title('Precision en fonction de k')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylim(0, 1)
        
        # Graphique 2: Recall@k
        axes[0, 1].plot(k_values, recalls, 's-', linewidth=2, markersize=8, color='green')
        axes[0, 1].fill_between(k_values, 
                               [r - results[k]['std_recall'] for k, r in zip(k_values, recalls)],
                               [r + results[k]['std_recall'] for k, r in zip(k_values, recalls)],
                               alpha=0.2, color='green')
        axes[0, 1].set_xlabel('k (nombre de recommandations)')
        axes[0, 1].set_ylabel('Recall@k')
        axes[0, 1].set_title('Rappel en fonction de k')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_ylim(0, 1)
        
        # Graphique 3: F1-score@k
        axes[1, 0].plot(k_values, f1_scores, '^-', linewidth=2, markersize=8, color='red')
        axes[1, 0].fill_between(k_values, 
                               [f - results[k]['std_f1'] for k, f in zip(k_values, f1_scores)],
                               [f + results[k]['std_f1'] for k, f in zip(k_values, f1_scores)],
                               alpha=0.2, color='red')
        axes[1, 0].set_xlabel('k (nombre de recommandations)')
        axes[1, 0].set_ylabel('F1-score@k')
        axes[1, 0].set_title('F1-score en fonction de k')
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_ylim(0, 1)
        
        # Graphique 4: Nombre d'utilisateurs
        axes[1, 1].bar(k_values, n_users, color='purple', alpha=0.7)
        axes[1, 1].set_xlabel('k')
        axes[1, 1].set_ylabel('Nombre d utilisateurs')
        axes[1, 1].set_title('Utilisateurs evalues par k')
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig('evaluation_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Tableau recapitulatif
        print("\n" + "="*80)
        print("TABLEAU RECAPITULATIF")
        print("="*80)
        print(f"{'k':<6} {'Precision@k':<15} {'Recall@k':<15} {'F1-score@k':<15} {'Utilisateurs':<12}")
        print("-"*80)
        
        for k in k_values:
            print(f"{k:<6} {results[k]['precision']:<15.3f} {results[k]['recall']:<15.3f} "
                  f"{results[k]['f1']:<15.3f} {results[k]['n_users']:<12}")
        
        # Analyse des resultats
        if results:
            best_k = max(results.keys(), key=lambda k: results[k]['f1'])
            print(f"\nMeilleur compromis: k = {best_k}")
            print(f"Precision@{best_k}: {results[best_k]['precision']:.3f}")
            print(f"Recall@{best_k}:    {results[best_k]['recall']:.3f}")
            print(f"F1-score@{best_k}:  {results[best_k]['f1']:.3f}")

# ============================================================================
# 6. PIPELINE PRINCIPAL
# ============================================================================

def run_main_pipeline():
    """Execute le pipeline principal"""
    print("=" * 80)
    print("SYSTEME DE RECOMMANDATION MUSICAL")
    print("Avec evaluation Precision@k et Recall@k")
    print("=" * 80)
    
    # Etape 1: Telechargement
    print("\nETAPE 1: Telechargement du dataset")
    print("-" * 50)
    
    downloader = MSDDownloader()
    data_dir = downloader.setup()
    
    # Etape 2: Preparation des donnees
    print("\nETAPE 2: Preparation des donnees")
    print("-" * 50)
    
    preparer = DataPreparer(data_dir)
    songs_df = preparer.load_metadata(limit=3000)
    songs_df = preparer.create_descriptions(songs_df)
    
    print(f"\nStatistiques des donnees:")
    print(f"  Nombre de chansons: {len(songs_df)}")
    print(f"  Artistes uniques: {songs_df['artist_name'].nunique()}")
    
    # Etape 3: Generation des embeddings
    print("\nETAPE 3: Generation des embeddings")
    print("-" * 50)
    
    embed_generator = EmbeddingGenerator()
    song_embeddings, song_id_to_embedding = embed_generator.generate_embeddings(songs_df)
    
    # Etape 4: Initialisation du systeme
    print("\nETAPE 4: Initialisation du systeme")
    print("-" * 50)
    
    recommender = MusicRecommender(songs_df, song_embeddings, song_id_to_embedding)
    
    # Etape 5: Demonstration
    print("\nETAPE 5: Demonstration du systeme")
    print("-" * 50)
    
    # Utilisateur de test
    sample_songs = songs_df.sample(6)
    user_history = []
    
    print("Historique utilisateur simule:")
    for i, (_, row) in enumerate(sample_songs.iterrows(), 1):
        views = np.random.randint(10, 100)
        user_history.append((row['track_id'], views))
        print(f"  {i}. {row['title']} - {row['artist_name']} ({views} vues)")
    
    # Generation de recommandations
    user_profile = recommender.create_user_profile(user_history)
    
    if user_profile:
        recommendations = recommender.recommend(user_profile, n_recommend=10, method='hybrid')
        
        if not recommendations.empty:
            print(f"\nTop 10 recommandations:")
            for i, (_, row) in enumerate(recommendations.iterrows(), 1):
                sim_score = row.get('similarity_score', 0)
                print(f"  {i}. {row['title']} - {row['artist_name']} (similarite: {sim_score:.3f})")
    
    # Etape 6: Evaluation
    print("\nETAPE 6: Evaluation du systeme")
    print("-" * 50)
    
    evaluator = SystemEvaluator(recommender, songs_df)
    
    # Generation des utilisateurs de test
    test_users = evaluator.generate_test_users(
        n_users=50,
        min_tracks=5,
        max_tracks=12,
        test_ratio=0.3
    )
    
    # Evaluation avec Precision@k et Recall@k
    print("\nEvaluation en cours...")
    evaluation_results = evaluator.evaluate_precision_recall_at_k(
        test_users, 
        k_values=[5, 10, 15, 20],
        alpha=0.6
    )
    
    # Visualisation des resultats
    if evaluation_results:
        evaluator.plot_results(
            evaluation_results,
            title="Evaluation du Systeme de Recommandation"
        )
    
    # Etape 7: Sauvegarde
    print("\nETAPE 7: Sauvegarde des resultats")
    print("-" * 50)
    
    # Sauvegarder les donnees
    songs_df.to_csv("songs_processed.csv", index=False)
    np.save("song_embeddings.npy", song_embeddings)
    
    # Sauvegarder les resultats d'evaluation
    if evaluation_results:
        with open('evaluation_results.json', 'w') as f:
            results_json = {}
            for k, v in evaluation_results.items():
                results_json[str(k)] = {
                    'precision': float(v['precision']),
                    'recall': float(v['recall']),
                    'f1': float(v['f1']),
                    'std_precision': float(v['std_precision']),
                    'std_recall': float(v['std_recall']),
                    'std_f1': float(v['std_f1']),
                    'n_users': int(v['n_users'])
                }
            json.dump(results_json, f, indent=2)
    
    print("\nFichiers sauvegardes:")
    print("  songs_processed.csv")
    print("  song_embeddings.npy")
    print("  evaluation_results.json")
    print("  evaluation_results.png")
    
    return {
        'recommender': recommender,
        'songs_df': songs_df,
        'evaluation_results': evaluation_results
    }

# ============================================================================
# 7. POINT D'ENTREE PRINCIPAL
# ============================================================================

if __name__ == "__main__":
    print("DEBUT DU PROGRAMME")
    print("="*80)
    
    try:
        # Executer le pipeline
        results = run_main_pipeline()
        
        print("\n" + "="*80)
        print("PROGRAMME TERMINE AVEC SUCCES")
        print("="*80)
        
        # Option pour tester interactivement
        if results is not None:
            try:
                test_choice = input("\nVoulez-vous tester le systeme interactivement? (oui/non): ").strip().lower()
                if test_choice in ['oui', 'o', 'yes', 'y']:
                    print("\n" + "="*80)
                    print("TEST INTERACTIF")
                    print("="*80)
                    
                    recommender = results['recommender']
                    songs_df = results['songs_df']
                    
                    print("\nSelectionnez 3-5 chansons:")
                    sample = songs_df.sample(min(15, len(songs_df)))
                    
                    for i, (_, row) in enumerate(sample.iterrows(), 1):
                        print(f"{i:2d}. {row['title'][:30]:30} - {row['artist_name'][:20]:20}")
                    
                    choices = input("\nNumeros (separes par des virgules): ").strip()
                    
                    try:
                        indices = [int(idx.strip()) - 1 for idx in choices.split(',')]
                        selected_rows = [sample.iloc[idx] for idx in indices if 0 <= idx < len(sample)]
                        
                        if len(selected_rows) >= 2:
                            user_history = []
                            print("\nVos selections:")
                            
                            for i, row in enumerate(selected_rows, 1):
                                views = int(input(f"  Ecoutes pour '{row['title']}'? (1-100): ") or "10")
                                user_history.append((row['track_id'], views))
                            
                            user_profile = recommender.create_user_profile(user_history)
                            
                            if user_profile:
                                recommendations = recommender.recommend(user_profile, n_recommend=8, method='hybrid')
                                
                                if not recommendations.empty:
                                    print(f"\nRecommandations personnalisees:")
                                    for i, (_, row) in enumerate(recommendations.iterrows(), 1):
                                        print(f"  {i}. {row['title']} - {row['artist_name']}")
                        else:
                            print("Au moins 2 chansons sont requises")
                            
                    except Exception as e:
                        print(f"Erreur: {e}")
            except:
                pass  # Ignorer les erreurs de l'interface interactive
        
    except KeyboardInterrupt:
        print("\nProgramme interrompu par l'utilisateur")
    except Exception as e:
        print(f"\nErreur durant l'execution: {e}")
        import traceback
        traceback.print_exc()
        print("\nConseils de depannage:")
        print("1. Assurez-vous d'avoir une connexion internet")
        print("2. Si scikit-learn pose probleme, installez-le manuellement:")
        print("   pip install scikit-learn==1.3.0")
        print("3. Pour sentence-transformers:")
        print("   pip install sentence-transformers")
        print("4. Verifiez que Python est en version 3.7 ou superieure")
